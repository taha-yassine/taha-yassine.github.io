---
title: Acceleration comes with speculation

description: An overview of some speculative decoding techniques.

pubDate: "2024-07-20"
---

Autoregressive language models work by generating tokens one after the other. The sequential nature of this process makes it inherently slow. Slow responses can make the experience of using these models frustrating, especially in settings where high throughput is essential, like in multi-turn dialogue systems. Intuitively, the most straightforward way to speed up the generation process is to alleviate hardware bottlenecks such as bandwidth or compute speed by chasing after the latest and greatest hardware. However, this approach is not sustainable, especially when a single company has a monopoly over the chip market{/* Add reference */}. Fortunatly, the LLM optimization space is full of low-hanging fruits waiting to be picked.{/* Add more info */}

One line of work that has been gaining traction recently is _speculative decoding_. The idea is to try to predict the next $n$ tokens in a cheap and often inaccurate way, with the hope that some of the predictions match the real tokens that the model would have generated. In this post I will give an overview of some speculative decoding techniques that I find interesting.
