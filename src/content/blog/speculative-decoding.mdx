---
title: Acceleration comes with speculation

description: Speeding up language models by guessing first and verifying later.

pubDate: "2025-11-02"
---
import Callout from '../../components/Callout.astro';
import Footnote from '../../components/Footnote.astro';

Autoregressive language models work by generating tokens one after the other. The sequential nature of this process makes it inherently slow. Slow responses can make the experience of using these models frustrating, especially in settings where high throughput is essential, like in multi-turn dialogue systems. Intuitively, the most straightforward way to speed up the generation process is to alleviate hardware bottlenecks such as bandwidth or compute speed by chasing after the latest and greatest hardware. However, this approach is not sustainable, especially when a single company has a monopoly over the chip market{/* Add reference */}. Fortunately, the LLM optimization space is full of low-hanging fruits waiting to be picked. Notable examples include model [distillation](https://en.wikipedia.org/wiki/Knowledge_distillation), sparsification (e.g., [MoE](https://huggingface.co/blog/moe)), [pruning](https://arxiv.org/abs/2305.11627), [quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization) and kernel fusion techniques (e.g., [FlashAttention](https://arxiv.org/abs/2205.14135)).

One line of work that has been gaining traction recently is _speculative decoding_. The idea is to try to predict the next $n$ tokens in a cheap and often inaccurate way, with the hope that some of the predictions match the real tokens that the model would have generated. In this post, I will give an overview of some speculative decoding techniques that I find interesting.

## Autoregressive decoding: a primer
Language models generate text by sequentially predicting the next token. Each new token is generated by conditioning on the previous tokens. This process is called _autoregressive decoding_. Mathematically, the probability of generating a sequence $x_1, x_2, \ldots, x_n$ is given by:

$$
\begin{align*}
p(x_1, x_2, \ldots, x_t) = p(x_{\leq t}) &= \prod_{i=1}^n p(x_i \mid x_{\leq i-1}) \\
&= p(x_1) \cdot p(x_2|x_1) \cdot \ldots \cdot p(x_t|x_{\leq t-1})
\end{align*}
$$

where $p(x_i \mid x_{\leq i-1})$ is the probability of generating the token $x_i$ given the previous tokens $x_1, \ldots, x_{i-1}$. Language models are nothing more than a function that, given a sequence of tokens, maps it to a conditional probability distribution of the next token over the vocabulary, i.e., $p(x_{t+1}=x|x_{\leq t})$ for every $x$ in $\mathcal{V}$. To decide which token to generate next, the model simply picks the one with the highest probability.<Footnote>Also known as _greedy decoding_.</Footnote>
$$
x_{t+1} = \arg\max_{x \in \mathcal{V}} p(x|x_{\leq t}).
$$

Notice how $x_{t+1}$ depends on all previous tokens $x_1, \ldots, x_{t}$. This means that to generate a sequence, we need to run the model $n$ times, once for each token. This process cannot be parallelized since each token depends on the previous ones.

{/* ## Speculative execution
Speculative decoding is inspired by a technique that comes from 


Speculative execution is used in modern CPUs to increase performance. It is a form of parallelism that allows the CPU to start executing instructions before they are ready, with the hope that some of the predictions will be correct. If a prediction is incorrect, the CPU can simply discard the instructions and revert to the previous state. This way, the CPU can still make progress even if some instructions are not yet ready.

Speculative decoding is a form of speculative execution for language models. The idea is to generate a draft sequence of tokens in a cheap and often inaccurate way, with the hope that some of the predictions match the real tokens that the model would have generated. In this post I will give an overview of some speculative decoding techniques that I find interesting. */}

## The basic idea
Speculative decoding is a technique that aims at accelerating autoregressive generation but still produce the same (or, in some cases, approximately the same) outputs. It relies on __three key observations__:
1. Tokens in a sequence can be processed in parallel at almost no additional latency compared to a single token.<Footnote>This is generally true in scenarios where the system is memory-bound rather than compute-bound.</Footnote>
2. For each (incomplete) sequence, the model outputs the probability distribution for the next token over the vocabulary.
3. Some tokens are easier to predict than others.

{/* ![Figure 1](../../assets/parallel_decoding.svg) */}

The first and second points suggest an interesting use case for language models: __sequence verification__. That is, for a given sequence, it becomes possible to verify in a single pass if the model is able to generate it according to some verification criterion<Footnote>This assumes greedy decoding.</Footnote>. A model that serves this purpose is called a _verifier_. In the greedy decoding setting for example, each token in the sequence should match the one predicted by the verifier. If at some point the verifier predicts a different token than what comes next in the sequence, that means that that token as well as all subsequent ones would not have been generated in the autoregressive setup.

If we can find a way to generate a draft sequence of tokens faster than what the model is capable of autoregressively, even if the draft is only partially accurate, then we can speed up the generation process by letting the model verify and decide what tokens to keep. The process is repeated until generation is complete. If the average number of accepted tokens is greater than one, then the throughput was indeed increased. In reality, this is not hard to achieve since a significant number of tokens are easy to predict without the need for powerful models. Take the following sequence: _"The sky is"_, then even your phone's autocorrect can predict that the next word is most probably _"blue"_. Building on this analogy, the drafter is your phone's autocorrect since it suggests the next word (*i.e*, a sequence of tokens of length one), and the verifier is you since you decide whether the suggestion matches what you intended to write or not.

<Callout title="Why are subsequent tokens dropped after a mismatch?">
When the target model deems a token incorrect during verification, all subsequent tokens should be discarded. This is simply because of the sequential relationship between tokens. Recall that, at any position $n$, any token $x_n$ depends on all previous tokens $x_{\leq n-1}$. If a draft token at position $t$ is incorrect, then all tokens at subsequent positions $>t$ are also incorrect.
It is also worth noting that since the target model verifies a token by comparing it to the actual next token that should follow, then when a rejection happens, we also know what to replace it with, meaning that we can resume drafting from the position of the second rejected token.
</Callout>

{/* Add a figure here. */}

In summary, a draft sequence of tokens is generated at high speed given some heuristic generation process, while the base model is used to verify the sequence in one go, with the hope that the first few tokens are correct. How many tokens are accepted is what determines the speed gain. The following pseudocode describes this process:

```python
def speculative_decoding(prefix, max_length, num_tokens):
    # Initialize output with the prefix
    output = prefix
    
    while len(output) < max_length:
        # Generate a draft continuation of `num_tokens` consecutive tokens
        draft = generate_draft(output, num_tokens=num_tokens)
        
        # Combine the current output with the draft continuation
        speculative_output = output + draft
        
        # Generate probabilities for each token using the availabe model
        probs = model.get_probabilities(speculative_output)
        
        # Compare draft tokens with model probabilities
        accepted_tokens = []
        for i, token in enumerate(draft):
            if token == argmax(probs[len(output) + i]):
                accepted_tokens.append(token)
            else:
                break
        
        # Update output with accepted tokens
        output += ''.join(accepted_tokens)
        
        # If no tokens were accepted, generate a single token using the target model
        if not accepted_tokens:
            next_token = argmax(model.get_probabilities(output)[-1])
            output += next_token
    
    return output
```

All speculative decoding methods rely on this base principle, and most differences come down to the process used to generate the draft sequences.

<Callout title="The off-by-one feature">
Since verification produces logits for the next token at each position, at least one token is always generated per iteration. If the first draft token is rejected, the correct token comes from the verification logits. If all draft tokens are accepted, an additional token can be generated from the last position's logits, giving an $n+1$ advantage when $n$ draft tokens are accepted.
</Callout>

## Batch verification with Tree Attention
Before continuing, let's take a look at a fundamental optimization trick that often comes up in speculative decoding and leads to significant improvements in terms of efficiency.

First, it is important to understand what attention masks are and what they are used for in transformer-based models. In a nutshell, they are used to specify which tokens are allowed to attend to which other tokens. For example, in causal language modeling, the attention mask is used to prevent the model from attending to future tokens. The associated attention matrix is lower triangular as result. In encoder models on the other hand, there are no such restrictions and the attention matrix is dense. There are many other types of attention masks to support different use cases, but the most important thing to understand is that they are a way of controlling which tokens are involved in the computations of the current token.

Let's go back to speculative decoding. We have seen that the usual pipeline is to iteratively use the drafter to generate a draft sequence, and then use the verifier to process it. What if we could generate multiple variations of the same draft at each iteration and verify all of them? This way, we could maximize our chances of generating correct tokens. The naive way of doing this is to verify each sequence independently, but this defeats the purpose of parallel verification since it requires as many passes as there are variations. One solution is to concatenate the sequences together to form a single long sequence and modify the attention mask to allow for tokens to attend to each other only within the same sequence. This is known as [sequence packing](https://huggingface.co/blog/sirluk/llm-sequence-packing). The resulting attention matrix is block diagonal with lower triangular blocks.

Packing sequences is an efficient way to verify multiple variations of the same draft at once, but we can do better. In general, variations occur at the token level at positions where multiple candidates could be valid. For example, given the prefix _"The quick brown fox..."_, both _"...jumped over the lazy dog"_ and _"...jumped over the wooden fence"_ would be valid drafts. Here, the first three words are the same in both cases. If we were to pack these sequences together, their corresponding tokens would be unnecessarily processed twice. While this isn't a big issue when the number of drafts is small (e.g., 2 here), it can become quite costly when that number increases. One way to address this is to represent the draft sequences as a tree where each node is a token. Just like in the case of packing, the tree should be converted to a single sequence for verification by flattening it. The order doesn't really matter<Footnote>This assumes correct positional embedding.</Footnote>. And again, the attention mask should be modified to control which tokens are allowed to attend to which depending on the branch they belong to in the tree. This type of attention mask is called [tree attention](https://arxiv.org/abs/2305.09781).

## Drafting
Let's now explore the different methods for generating draft sequences, starting from the most straightforward approaches and gradually moving to more sophisticated ones.

### Prompt lookup
[Prompt lookup](https://github.com/apoorvumang/prompt-lookup-decoding) is maybe the simplest way to generate draft sequences. It's based on the intuition that, in many cases, there is an overlap between the past context and the text to be generated. For example, if _"New York"_ appears in the context, and at any point in the future the model generates the token _"New"_, then it's very likely that the next one is going to be _"York"_.

In practice, prompt lookup works by taking the last $n$-gram of tokens and sliding them over the past context until a match is found. If a match is found at a given position, the following tokens (up to a maximum length) are used as the draft sequence, and generation is resumed from that point. If not, the process is repeated with a smaller $n$-gram.

{/* Add a figure here. */}

Prompt lookup works great with tasks that have a high degree of repetition. Examples include code generation, text summarization, multi-turn chat, etc. It can also be easily extended to search for similar tokens in previously cached sessions and in retrieved documents in RAG systems ([LLMA](https://arxiv.org/abs/2304.04487)).

### Retrieval-based drafting
[Retrieval-based drafting](https://arxiv.org/abs/2311.08252) takes prompt lookup a step further. Instead of just searching within the current context, this approach can retrieve relevant text segments from a larger external datastore. While the idea remains the same, the main challenge lies in designing effective retrieval mechanisms that can quickly identify relevant text segments across a large batch of candidates.

### Using a small model
A natural choice for generating draft sequences is to use a separate smaller model. Smaller models use less parameters and are thus faster to run. While size is directly correlated with the model's capabilities, i.e., smaller models are supposedly less powerful than larger ones, this shouldn't matter in the speculative decoding setting since the base model always verifies the generated sequences. Anything that's generated and accepted is guaranteed to be correct. **Speculative decoding is lossless.**

While in theory any smaller model can be used as a draft model, finding an adequate one can sometimes be challenging. First, its vocabulary must match the base model's vocabulary to ensure a full coverage of the tokens<Footnote>Method to use models with different tokenizers: https://huggingface.co/blog/universal_assisted_generation</Footnote>. Second, the draft model must be small enough to ensure that the generation process is fast, and accurate enough to generate a good draft sequence. This makes smaller models from the same family particularly suitable since they usually share many characteristics such as the tokenizer, architecture and training recipe. Distilled models also make for a great choice since they are trained to match the base model's distribution, making their drafts more likely to be accepted.

### Self-drafting
Draft-model based speculative decoding works great when correctly implemented. However, some of its downsides are that it has a higher memory footprint and that it requires synchronization between the draft model and the base model which can be tricky to get right, especially in a distributed setting. It also assumes that memory bandwidth is the main bottleneck, while not fully accounting for the additional FLOPS required.

Instead, it has been proposed to use the target model itself to generate draft sequences by extending it to allow for multiple tokens to be generated at once. This way, the model can generate a draft sequence itself in a single pass.

#### Parallel decoding
The most straightforward way to make a model guess for multiple successive tokens in one pass is to supply it with a dummy sequence at initialization and have it iteratively refine it. The dummy sequence usually consists of `[PAD]` tokens. The model will then produce a draft continuation token for each `[PAD]` token. This draft can then be plugged-in for the next iteration of drafting. This procedure, inspired by the [Jacobi method](https://en.wikipedia.org/wiki/Jacobi_method), is then repeated until all tokens are verified. At worst, it takes as many iterations to complete as there are tokens, which is equivalent to autoregressive decoding. The algorithm relies on the fact that sometimes multiple tokens can be correctly predicted in a single pass.

{/* #### Lookahead decoding
While parallel decoding allows for predicting multiple tokens in a single pass without modifying the verifier, it is often less accurate than other methods. It particularly struggles with correctly positioning the tokens in the sequence. As a result, it might correctly predict a subsequence of tokens but place them at the wrong position. Building on this observation, [lookahead decoding](https://lmsys.org/blog/2023-11-21-lookahead-decoding/) proposes to keep a cache of $n$-grams  */}

#### Drafting heads
While parallel and lookahead decoding are straightforward and perfectly valid implementations of speculative decoding, using the verifier for drafting in such a way with no further adaptations has its limitations in terms of the quality of the generated drafts, which in turn limits the speed gain. As a middle ground between this and using a whole standalone, albeit small {/*check if worded correctly*/}, model for drafting, it has been proposed to augment verifiers with drafting heads that consist of neural modules that feed off the last hidden state before the LM head to produce draft tokens. [Medusa](https://arxiv.org/pdf/2401.10774) is one example.

Specifically, transformer-based language models, and especially decoders, consist of several successive transformer blocks that do the bulk of the calculations. Each block produces a _hidden state_ for each token. Let's denote the hidden state produced by the last block for token $x_t$ as $h_t$. When decoding, $h_t$ goes through the LM head which consists of a linear layer that maps it to the logits and a softmax function that turns them into a probability distribution over the vocabulary. This means that $h_t$ can be seen as a semantically rich latent representation of the sequence $x_{\leq t}$.

Building on this intuition, Medusa proposes to implement drafting heads as simple MLPs that take as input the last hidden state $h_t$ and train them to predict the token at their assigned position. The $k$-th drafting head is defined as
$$
p_{\text{draft}}(\hat{x}_{t+1+k}\mid x_{\leq t}, x_{t+1}, \hat{x}_{t+2}, \ldots, \hat{x}_{t+k-1}) = p_{\text{draft}}(\hat{x}_{t+1+k}\mid x_{\leq t})
$$

The drafting heads can also be seen as a separate small draft model that takes as input the last hidden state of the base model. From this perspective, it becomes clear that the drafted sequence still needs to be processed by the verifier to only keep the tokens that are accepted.

## Sampling
When the generation heuristic exposes the logits associated with the underlying conditional distribution, speculative decoding can be extended beyond greedy decoding to the sampling setting. This is particularly important for creative applications where diversity in outputs is desired.

The key insight is that speculative sampling can be framed as a form of [rejection sampling](https://en.wikipedia.org/wiki/Rejection_sampling). Instead of accepting only the token with maximum probability, we can accept draft tokens according to their probability under the target model's distribution. This maintains the statistical properties of the target model while still benefiting from the acceleration provided by the draft model.

The acceptance probability for a draft token $x_i$ becomes:
$$
\min\left(1, \frac{p_{\text{target}}(x_i|x_{<i})}{p_{\text{draft}}(x_i|x_{<i})}\right)
$$

If a token is rejected, we resample from the following distribution:
$$
\frac{\max(0,p_{\text{target}}(x_i|x_{<i})-p_{\text{draft}}(x_i|x_{<i}))}{\sum_{x}\max(0,p_{\text{target}}(x|x_{<i})-p_{\text{draft}}(x|x_{<i}))}
$$

This approach ensures that the final distribution matches the target model's distribution exactly. It is also compatible with the usual sampling techniques such as top-k sampling, nucleus sampling and temperature adjustment.

## Bonus: Promising research directions
- **Dynamic speculative decoding**: Instead of using fixed draft lengths, dynamic approaches adapt the drafting strategy based on real-time feedback. This could involve adjusting the number of draft tokens based on acceptance rates and incoming requests.

- **Hierarchical speculative decoding**: Applying speculative decoding by stacking verifier models of increasing sizes.

- **Speculative decoding for multimodal models**: Extending these techniques to vision-language models where the drafting might involve both text and image tokens.

- **Multi-draft speculative decoding**: _TODO_.

## Conclusion

Speculative decoding offers a promising way to speed up language model generation. It's a simple technique that can be applied to any autoregressive model with little to no modification, and it can be used in tandem with a wide range of optimization techniques. Since it's lossless, it should be considered as a foundational optimization for any production LLM system.

The beauty of speculative decoding lies in its versatility. From simple n-gram matching to sophisticated learned drafting heads, the core principle remains the same: use cheap predictions to accelerate expensive verification. As models continue to grow in size and capability, techniques like speculative decoding will become increasingly essential for making AI systems responsive enough for real-world applications.

Looking forward, the field is ripe with opportunities for innovation, from dynamic adaptation strategies to cross-modal applications. The next generation of efficient AI systems will likely combine speculative decoding with other optimization techniques to achieve both high quality and high throughput generation.

## References
- [vLLM Blog: How Speculative Decoding Boosts vLLM Performance by up to 2.8x](https://blog.vllm.ai/2024/10/17/spec-decode.html)
- _TODO_
